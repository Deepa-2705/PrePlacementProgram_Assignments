{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce34de7-436c-418b-be8e-624e426cdb16",
   "metadata": {},
   "source": [
    "## Naive Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cb019-558f-45c2-b3fb-547a9e7cfba6",
   "metadata": {},
   "source": [
    "Que 1. What is the Naive Approach in machine learning?\n",
    "\n",
    "Ans: A  naive approach consists of calculating a histogram of θ ( k ) angles, assuming the accumulation of points corresponding to the directions of interest will result in visible peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa0e99-0ee4-4b2b-bf90-6772a6db5407",
   "metadata": {},
   "source": [
    "Que 2. Explain the assumptions of feature independence in the Naive Approach.\n",
    "\n",
    "Ans: Here are some assumptions that the Naive Bayers algorithm makes: The main assumption is that it assumes that the features are conditionally independent of each other. Each of the features is equal in terms of weightage and importance. The algorithm assumes that the features follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1e87f-9c82-46bc-85ad-4e73facf8d51",
   "metadata": {},
   "source": [
    "Que 3. How does the Naive Approach handle missing values in the data?\n",
    "\n",
    "Ans: Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505603f-82a7-4e6c-85e5-87cf645967a8",
   "metadata": {},
   "source": [
    "Que 4. What are the advantages and disadvantages of the Naive Approach?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "Advantages:\n",
    "    \n",
    "1) This algorithm works quickly and can save a lot of time. \n",
    "\n",
    "2) Naive Bayes is suitable for solving multi-class prediction problems. \n",
    "\n",
    "3) If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data. \n",
    "\n",
    "4) Naive Bayes is better suited for categorical input variables than numerical variables.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1) Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life. This limits the applicability of this algorithm in real-world use cases.\n",
    "\n",
    "2) This algorithm faces the ‘zero-frequency problem’ where it assigns zero probability to a categorical variable whose category in the test data set wasn’t available in the training dataset. It would be best if you used a smoothing technique to overcome this issue.\n",
    "\n",
    "3) Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4fcea-b30d-4f13-9647-a54f63139ded",
   "metadata": {},
   "source": [
    "Que 5. Can the Naive Approach be used for regression problems? If yes, how?\n",
    "\n",
    "Ans: Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems, though with some modifications, it can also be used for solving regression problems.\n",
    "\n",
    "The comparison with linear regression depends on the error measure: for one measure naive Bayes performs similarly, while for another it is worse. We also show that standard naive Bayes ap- plied to regression problems by discretizing the target value performs similarly badly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65cf85f-b8da-4569-8f21-e87171055546",
   "metadata": {},
   "source": [
    "Que 6. How do you handle categorical features in the Naive Approach?\n",
    "\n",
    "Ans: Different Approaches to Handle Categorical Data\n",
    "\n",
    "· One Hot Encoding\n",
    "\n",
    "· One Hot Encoding with multiple categories\n",
    "\n",
    "· Ordinal Number Encoding\n",
    "\n",
    "· Count or Frequency Encoding\n",
    "\n",
    "· Target guided Ordinal Encoding\n",
    "\n",
    "· Mean Ordinal Encoding\n",
    "\n",
    "· Probability Ratio Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547d1c5-18d8-4f26-a835-7ecbef50ab5b",
   "metadata": {},
   "source": [
    "Que 7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
    "\n",
    "Ans: Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ac773-222a-4e4b-8647-805e3fda8266",
   "metadata": {},
   "source": [
    "Que 8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
    "\n",
    "Ans : There are many techniques that may be used to address an imbalanced classification problem, such as resampling the training dataset and developing customized version of machine learning algorithms.\n",
    "\n",
    "1) Adjust some threshold value that control the number of examples labelled true or false\n",
    "\n",
    "2) For example, if concentration of certain protein above α% signifies a disease, different values of α yield different final TPR and FPR values. The threshold values can be simply determined in a way similar to grid search; label training examples with different threshold values, train classifiers with different sets of labelled examples, run the classifier on the test data, compute FPR values, and select the threshold values that cover low (close to 0) and high (close to 1) FPR values, i.e., close to 0, 0.05, 0.1, ..., 0.95, 1\n",
    "\n",
    "3) Generate many sets of annotated examples\n",
    "\n",
    "4) Run the classifier on the sets of examples\n",
    "\n",
    "5) Compute a (FPR, TPR) point for each of them\n",
    "\n",
    "6) Draw the final ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f00e5e-4e23-44dd-8c22-d2364691bbc5",
   "metadata": {},
   "source": [
    "Que 9. Give an example scenario where the Naive Approach can be applied.\n",
    "\n",
    "Ans: Some best examples of the Naive Bayes Algorithm are sentimental analysis, classifying new articles, and spam filtration. Classification algorithms are used for categorizing new observations into predefined classes for the uninitiated data. The Naive Bayes Algorithm is known for its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc9de7-797a-4d9a-978e-f480c1d4ccbc",
   "metadata": {},
   "source": [
    "## KNN:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfb0c9-167b-4f55-a2e9-0a4593c97f58",
   "metadata": {},
   "source": [
    "Que 10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
    "\n",
    "Ans: The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d22e3d-b901-4044-9de1-2c5cb05c6b09",
   "metadata": {},
   "source": [
    "Que 11. How does the KNN algorithm work?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "1) First, the distance between the new point and each training point is calculated.\n",
    "\n",
    "2) The closest k data points are selected (based on the distance). ...\n",
    "\n",
    "3) The average of these data points is the final prediction for the new point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d46fc-50e0-4ff0-8101-8c15b6fffbd3",
   "metadata": {},
   "source": [
    "Que 12. How do you choose the value of K in KNN?\n",
    "\n",
    "Ans: The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41d929-aa45-4173-9b45-85bc363c25da",
   "metadata": {},
   "source": [
    "Que 13. What are the advantages and disadvantages of the KNN algorithm?\n",
    "\n",
    "Ans: \n",
    "\n",
    "    * Advantages\t\n",
    "\n",
    "1) Simple and Easy to Understand\n",
    "\n",
    "2) Non-parametric\t\n",
    "\n",
    "3) No Training Required\t\n",
    "\n",
    "4) Can Handle Large Datasets\t\n",
    "\n",
    "5) Accurate and Effective\n",
    "\n",
    "\n",
    "\n",
    "   * Disadvantages\n",
    "\n",
    "1) Sensitive to Outliers\n",
    "\n",
    "2) Computationally Expensive\n",
    "\n",
    "3) Requires Good Choice of K\n",
    "\n",
    "4) Limited to Euclidean Distance\n",
    "\n",
    "5) Imbalanced Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea29b6-0e6e-449a-9f60-82522b0d88b7",
   "metadata": {},
   "source": [
    "Que 14. How does the choice of distance metric affect the performance of KNN?\n",
    "\n",
    "Ans: In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5e0fa-ab6e-418c-83fa-30aebd5dd48b",
   "metadata": {},
   "source": [
    "Que 15. Can KNN handle imbalanced datasets? If yes, how?\n",
    "\n",
    "Ans:  Yes ,KNN handle imbalanced datasets.    \n",
    "In k Nearest Neighbor (kNN) classifier, a query instance is classified based on the most frequent class of its nearest neighbors among the training instances. In imbalanced datasets, kNN becomes biased towards the majority instances of the training space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aaf820-db34-4b7f-b389-4113acc1b089",
   "metadata": {},
   "source": [
    "Que 16. How do you handle categorical features in KNN?\n",
    "\n",
    "Ans: \n",
    "It doesn't handle categorical features. This is a fundamental weakness of kNN. kNN doesn't work great in general when features are on different scales. This is especially true when one of the 'scales' is a category label. You have to decide how to convert categorical features to a numeric scale, and somehow assign inter-category distances in a way that makes sense with other features (like, age-age distances...but what is an age-category distance?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd736c-f6d0-4664-a4d8-a671d01c71a7",
   "metadata": {},
   "source": [
    "Que 17. What are some techniques for improving the efficiency of KNN?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "1) Choose the right k\n",
    "\n",
    "The number of neighbors, or k, is a crucial parameter for KNN. It affects both the complexity and the bias-variance trade-off of the model. If k is too small, the model can be sensitive to noise and outliers, resulting in high variance and overfitting. If k is too large, the model can be influenced by irrelevant neighbors, resulting in high bias and underfitting. \n",
    "\n",
    "2) Preprocess the data\n",
    "\n",
    "KNN relies on the distance between data points to make predictions, so it is essential to preprocess the data in order to make sure that the distance metric is both meaningful and consistent. This could involve scaling or normalizing features to prevent large values from dominating, encoding categorical variables with numerical values, using filters, smoothing, or trimming techniques to reduce noise and outliers, and handling missing values through imputation, dropping, or ignoring them.\n",
    "\n",
    "3) Reduce the dimensionality\n",
    "\n",
    "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model. To reduce the dimensionality, feature selection, feature extraction, and feature engineering are all viable options.\n",
    "\n",
    "4) Use an index structure\n",
    "\n",
    "Finally, you can use an index structure to speed up the search for the nearest neighbors. An index structure is a data structure that organizes the data points in a way that allows for faster and easier queries. For example, you can use a tree-based index, such as a KD-tree or a ball-tree, to partition the data into regions and subregions based on some criteria. Then, you can use a pruning strategy to eliminate the regions that are far from the query point and focus on the ones that are closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33819801-5b25-4da1-8ad2-2eb692b0362b",
   "metadata": {},
   "source": [
    "Que 18. Give an example scenario where KNN can be applied.\n",
    "\n",
    "Ans: With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b90154-7fe4-4c9a-b7ef-37d35a4aa0d8",
   "metadata": {},
   "source": [
    "## Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482925c4-d7c5-41ae-a977-5f7abe60da6d",
   "metadata": {},
   "source": [
    "Que 19. What is clustering in machine learning?\n",
    "\n",
    "Ans: Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dcdd4-1ea8-454f-ad61-793405eda55e",
   "metadata": {},
   "source": [
    "Que 20. Explain the difference between hierarchical clustering and k-means clustering.\n",
    "\n",
    "Ans: k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce0ee8-1911-4c51-802b-aeaaf64b32c7",
   "metadata": {},
   "source": [
    "Que 21. How do you determine the optimal number of clusters in k-means clustering?\n",
    "\n",
    "Ans : Average silhouette method\n",
    "\n",
    "1) Compute clustering algorithm (e.g., k-means clustering) for different values of k. \n",
    "\n",
    "2) For each k, calculate the average silhouette of observations \n",
    "\n",
    "3) Plot the curve of avg. \n",
    "\n",
    "4) The location of the maximum is considered as the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79941ec5-eea8-40b6-8918-938124732e39",
   "metadata": {},
   "source": [
    "Que 22. What are some common distance metrics used in clustering?\n",
    "\n",
    "Ans: Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527556c2-a765-4888-801d-efe3973e013f",
   "metadata": {},
   "source": [
    "Que 23. How do you handle categorical features in clustering?\n",
    "\n",
    "Ans: One way to handle categorical variables is to use one-hot encoding. One-hot encoding transforms categorical variables into a set of binary features, where each feature represents a distinct category. For example, suppose we have a categorical variable “color” that can take on the values red, blue, or yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac4008-eaa9-4388-b043-fa9ca8c1599b",
   "metadata": {},
   "source": [
    "Que 24. What are the advantages and disadvantages of hierarchical clustering?\n",
    "\n",
    "Ans: The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets. \n",
    "\n",
    "And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e5090-5e42-4038-b95f-d42e53aaef98",
   "metadata": {},
   "source": [
    "Que 25. Explain the concept of silhouette score and its interpretation in clustering.\n",
    "\n",
    "Ans:\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "The value of the silhouette coefﬁcient is between [-1, 1]. A score of 1 denotes the best, meaning that the data point i is very compact within the cluster to which it belongs and far away from the other clusters. The worst value is -1. Values near 0 denote overlapping clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ed273-ac65-4f0e-a820-8365a1e59337",
   "metadata": {},
   "source": [
    "Que 26. Give an example scenario where clustering can be applied.\n",
    "\n",
    "Ans: For instance, clustering can be used to identify several types of depression. Cluster analysis is also used to identify patterns in the spatial or temporal allocation of a disease. Business − Businesses collect huge amounts of data on current and potential users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbd8e9-8d46-4686-b1c8-91d17b158021",
   "metadata": {},
   "source": [
    "## Anomaly Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de97efa-3aee-4162-9580-d9a5b16e95f3",
   "metadata": {},
   "source": [
    "Que 27. What is anomaly detection in machine learning?\n",
    "\n",
    "Ans: Anomaly detection is an artificial intelligence technique used to determine whether values in a series are within expected parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e13a3a-47e5-42be-b26b-5a003dc222f6",
   "metadata": {},
   "source": [
    "Que 28. Explain the difference between supervised and unsupervised anomaly detection.\n",
    "\n",
    "Ans: \n",
    "Supervised learning harnesses the power of labeled data to train models that can make accurate predictions or classifications.\n",
    "\n",
    "In contrast, unsupervised learning focuses on uncovering hidden patterns and structures within unlabeled data, using techniques like clustering or anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273eac4-7bb4-4250-93c4-cdb003b29adb",
   "metadata": {},
   "source": [
    "Que 29. What are some common techniques used for anomaly detection?\n",
    "\n",
    "Ans: some common techniques used for anomaly detection are: \n",
    "    \n",
    "1) Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
    "\n",
    "2) Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests.\n",
    "\n",
    "3) Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data\n",
    "\n",
    "4) One-class support vector machines\n",
    "\n",
    "5) Replicator neural networks, autoencoders, variational autoencoders, long short-term memory neural networks\n",
    "\n",
    "6) Bayesian networks\n",
    "\n",
    "7) Hidden Markov models (HMMs)\n",
    "\n",
    "8) Minimum Covariance Determinant\n",
    "\n",
    "9) Clustering: Cluster analysis-based outlier detection\n",
    "\n",
    "10) Deviations from association rules and frequent itemsets\n",
    "\n",
    "11) Fuzzy logic-based outlier detection\n",
    "\n",
    "12) Ensemble techniques, using feature bagging, score normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f0653-eb01-4319-85f9-3947da3d6b37",
   "metadata": {},
   "source": [
    "Que 30. How does the One-Class SVM algorithm work for anomaly detection?\n",
    "\n",
    "Ans: One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091008d-6340-4539-a114-e73b68409146",
   "metadata": {},
   "source": [
    "Que 31. How do you choose the appropriate threshold for anomaly detection?\n",
    "\n",
    "Ans: \n",
    "1) In the Upper Limit and Lower Limit fields, specify the upper and lower threshold limits.\n",
    "\n",
    "2) In the Consecutive Occurrences spinner, specify the number of consecutive threshold violations that must occur before an anomaly and an event is generated.\n",
    "\n",
    "3) In the Type drop-down list, specify the threshold type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf5ae0-4e3a-4efc-bee3-53ee2b4b3c5a",
   "metadata": {},
   "source": [
    "Que 32. How do you handle imbalanced datasets in anomaly detection?\n",
    "\n",
    "Ans: Techniques to Handle Imbalanced Data\n",
    "\n",
    "1) Approach to deal with the imbalanced dataset problem\n",
    "\n",
    "2) Choose Proper Evaluation Metric. The accuracy of a classifier is the total number of correct predictions by the classifier divided by the total number of predictions. \n",
    "\n",
    "3) Resampling (Oversampling and Undersampling) \n",
    "\n",
    "4) SMOTE. \n",
    "\n",
    "5) BalancedBaggingClassifier. \n",
    "\n",
    "6) Threshold moving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f18c2-dc98-4aca-acd1-10af36f1b7eb",
   "metadata": {},
   "source": [
    "Que 33. Give an example scenario where anomaly detection can be applied.\n",
    "\n",
    "Ans: a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e8db9-ecf0-4b1d-b62f-975b1743224b",
   "metadata": {},
   "source": [
    "## Dimension Reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc145768-bb8b-4e15-b145-96d050b9cba7",
   "metadata": {},
   "source": [
    "Que 34. What is dimension reduction in machine learning?\n",
    "\n",
    "Ans: Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e43bd-cae4-4b0f-b4d2-13df01065fbd",
   "metadata": {},
   "source": [
    "Que 35. Explain the difference between feature selection and feature extraction.\n",
    "\n",
    "Ans: The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475cca2-f2ba-4ec5-abbb-911db08d7d48",
   "metadata": {},
   "source": [
    "Que 36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
    "\n",
    "Ans: PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2f8f4-5219-4467-b858-7f778ad145c5",
   "metadata": {},
   "source": [
    "Que 37. How do you choose the number of components in PCA?\n",
    "\n",
    "Ans: If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092cd87-4a51-4057-9f60-bf7c3df3b944",
   "metadata": {},
   "source": [
    "Que 38. What are some other dimension reduction techniques besides PCA?\n",
    "\n",
    "Ans: There are several techniques for dimensionality reduction, including principal component analysis (PCA), singular value decomposition (SVD), and linear discriminant analysis (LDA). Each technique uses a different method to project the data onto a lower-dimensional space while preserving important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2dfb4-a845-4830-b7ac-0bcc741097f7",
   "metadata": {},
   "source": [
    "Que 39. Give an example scenario where dimension reduction can be applied.\n",
    "\n",
    "Ans: Practical Example of Dimensionality Reduction\n",
    "1) Missing values ratio.\n",
    "\n",
    "2) High correlation filter.\n",
    "\n",
    "3) Recursive Feature Elimination (RFE)\n",
    "\n",
    "4) Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23b20c-84dd-4f95-81c4-ed88406a0598",
   "metadata": {},
   "source": [
    "## Feature Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4c11d-cf6c-4046-8daf-d778324144b4",
   "metadata": {},
   "source": [
    "Que 40. What is feature selection in machine learning?\n",
    "\n",
    "Ans: Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af93c56-0d49-471a-a63b-26f5feb6a7c2",
   "metadata": {},
   "source": [
    "Que 41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
    "\n",
    "Ans: \n",
    "Filter method is faster and useful when there are more number of features.\n",
    "\n",
    "Wrapper method gives better performance \n",
    "\n",
    "while the embedded method lies in between the other two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e35380-c87b-481b-bebd-c0137cecde5d",
   "metadata": {},
   "source": [
    "Que 42. How does correlation-based feature selection work?\n",
    "\n",
    "Ans: A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d0dbe-e97e-4df4-8b01-d7ffdc925acc",
   "metadata": {},
   "source": [
    "Que 43. How do you handle multicollinearity in feature selection?\n",
    "\n",
    "Ans: Addressing multicollinearity: \n",
    "\n",
    "1) Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation. ...\n",
    "\n",
    "2) More data. ...\n",
    "\n",
    "3) Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). ...\n",
    "\n",
    "4) Centering the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212f78-1a92-4d60-b3af-57b62ab7fa10",
   "metadata": {},
   "source": [
    "Que 44. What are some common feature selection metrics?\n",
    "\n",
    "Ans: There are three general classes of feature selection algorithms: Filter methods, wrapper methods and embedded methods. The role of feature selection in machine learning is, 1. To reduce the dimensionality of feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5027567-57d6-4415-b8cb-da22b1e9abda",
   "metadata": {},
   "source": [
    "Que 45. Give an example scenario where feature selection can be applied.\n",
    "\n",
    "Ans: Below are some real-life examples of feature selection: Mammographic image analysis. Criminal behavior modeling. Genomic data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f69c4-b5af-4863-97f7-9811f695efc4",
   "metadata": {},
   "source": [
    "## Data Drift Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470c977-7682-467f-8cb2-bc1435c9805e",
   "metadata": {},
   "source": [
    "Que 46. What is data drift in machine learning?\n",
    "\n",
    "Ans: Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdb8c8-3c26-416d-b968-db2d152a71b6",
   "metadata": {},
   "source": [
    "Que 47. Why is data drift detection important?\n",
    "\n",
    "Ans: Data drift is an important concept in machine learning because it can cause a trained model to become less accurate over time, leading to incorrect predictions and suboptimal decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fafee-0f70-45a5-9e2f-25db4c545e4b",
   "metadata": {},
   "source": [
    "Que 48. Explain the difference between concept drift and feature drift.\n",
    "\n",
    "Ans: \n",
    "Concept drift is a type of model drift where the properties of the dependent variable changes. The fraudulent model above is an example of concept drift, where the classification of what is 'fraudulent' changes. Data drift is a type of model drift where the properties of the independent variable(s) change(s).\n",
    "\n",
    "Design occurs at the concept level, whereas features build on top of concepts. This can be a little abstract, but I recently ran into a situation where the presence of a concept in one area allowed for feature development, while the absence of a concept in another area made change difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803c442-c767-424a-9c54-5bd8b9aaf351",
   "metadata": {},
   "source": [
    "Que 49. What are some techniques used for detecting data drift?\n",
    "\n",
    "Ans: Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b114c5-00d4-4d7e-b38d-47000932e703",
   "metadata": {},
   "source": [
    "Que 50. How can you handle data drift in a machine learning model?\n",
    "\n",
    "Ans: Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eabcae-ff4e-42e4-a012-f320effebef6",
   "metadata": {},
   "source": [
    "## Data Leakage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a22ea1-dedb-4db8-b79f-798da9cbb365",
   "metadata": {},
   "source": [
    "Que 51. What is data leakage in machine learning?\n",
    "\n",
    "Ans : Data leakage is when information from outside the training dataset is used to create the model.\n",
    "Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e4f0d-3e2d-4c66-8e80-3433dda05154",
   "metadata": {},
   "source": [
    "Que 52. Why is data leakage a concern?\n",
    "\n",
    "Ans: It is a serious problem for at least 3 reasons:\n",
    "\n",
    "1) It is a problem if you are running a machine learning competition. Top models will use the leaky data rather than be good general model of the underlying problem.\n",
    "\n",
    "2) It is a problem when you are a company providing your data. Reversing an anonymization and obfuscation can result in a privacy breach that you did not expect.\n",
    "\n",
    "3) It is a problem when you are developing your own predictive models. You may be creating overly optimistic models that are practically useless and cannot be used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f879b-e9fa-4583-9c24-9960d1a18cdc",
   "metadata": {},
   "source": [
    "Que 53. Explain the difference between target leakage and train-test contamination.\n",
    "\n",
    "Ans: \n",
    "\n",
    "* Target leakage\n",
    "\n",
    "Target leakage can happen when a variable that is not a feature is being used to predict the target. Target leakage happens more frequently with complex datasets. For instance, if the training dataset was normalized or standardized by using missing value imputation (min, max, mean).\n",
    "\n",
    "* Train Test Contamination\n",
    "\n",
    "This happens when we unknowingly or subtly pass information from our train dataset to our validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85451c-595d-46f9-b5a6-08394d6626dc",
   "metadata": {},
   "source": [
    "Que 54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
    "\n",
    "Ans: An easy way to know you have data leakage is if you are achieving performance that seems a little too good to be true.\n",
    "Like you can predict lottery numbers or pick stocks with high accuracy.\n",
    "    \n",
    "*Preventions: \n",
    "\n",
    "1) Temporal Cutoff. Remove all data just prior to the event of interest, focusing on the time you learned about a fact or observation rather than the time the observation occurred.\n",
    "\n",
    "2) Add Noise. Add random noise to input data to try and smooth out the effects of possibly leaking variables.\n",
    "\n",
    "3) Remove Leaky Variables. Evaluate simple rule based models line OneR using variables like account numbers and IDs and the like to see if these variables are leaky, and if so, remove them. If you suspect a variable is leaky, consider removing it.\n",
    "\n",
    "4) Use Pipelines. Heavily use pipeline architectures that allow a sequence of data preparation steps to be performed within cross validation folds, such as the caret package in R and Pipelines in scikit-learn.\n",
    "\n",
    "5) Use a Holdout Dataset. Hold back an unseen validation dataset as a final sanity check of your model before you use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0c808-f4a9-478f-8320-a9275aec540e",
   "metadata": {},
   "source": [
    "Que 55. What are some common sources of data leakage?\n",
    "\n",
    "Ans: A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e149d-0590-4ae0-acd8-c0f522c864a4",
   "metadata": {},
   "source": [
    "Que 56. Give an example scenario where data leakage can occur.\n",
    "\n",
    "Ans: Data leakage occurs when sensitive data gets unintentionally exposed to the public in transit, at rest, or in use. Here are common examples: Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb733e-18de-4725-a3f5-694e77fd6e8a",
   "metadata": {},
   "source": [
    "## Cross Validation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58e3d1-03cb-4feb-9ea6-5b45eea5d68b",
   "metadata": {},
   "source": [
    "Que 57. What is cross-validation in machine learning?\n",
    "\n",
    "Ans: Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68672e-0006-44b3-84de-cc07228a1901",
   "metadata": {},
   "source": [
    "Que 58. Why is cross-validation important?\n",
    "\n",
    "Ans: The purpose of cross–validation is to test the ability of a machine learning model to predict new data. It is also used to flag problems like overfitting or selection bias and gives insights on how the model will generalize to an independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1af5a-7526-4756-a704-7af7b0de969e",
   "metadata": {},
   "source": [
    "Que 59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
    "\n",
    "Ans: KFold devides the dataset into k folds. Where as Stratified ensures that each fold of dataset has the same proportion of observations with a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b1089-aa03-4f83-8d5c-98b74f954f54",
   "metadata": {},
   "source": [
    "Que 60. How do you interpret the cross-validation results?\n",
    "\n",
    "Ans: Cross validation is a technique that allows us to produce test set like scoring metrics using the training set. That is, it allows us to simulate the effects of “going out of sample” using just our training data, so we can get a sense of how well our model generalizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
