{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9296b9-b7c3-4cbb-9a07-a5c0861d69c2",
   "metadata": {},
   "source": [
    "### General Linear Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24ee29-3157-476a-8714-a51b403ab7a5",
   "metadata": {},
   "source": [
    "Que1: What is the purpose of the General Linear Model (GLM)?\n",
    "\n",
    "Ans: GLM models allow us to build a linear relationship between the response and predictors, even though their underlying relationship is not linear. This is made possible by using a link function, which links the response variable to a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e146b4-9ca9-427d-86d4-371a06b28739",
   "metadata": {},
   "source": [
    "Que2: What are the key assumptions of the General Linear Model?\n",
    "\n",
    "Ans: The general linear model fitted using ordinary least squares (which includes Student's t test, ANOVA, and linear regression) makes four assumptions: linearity, homoskedasticity (constant variance), normality, and independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316e69c-d3e8-4035-afa1-b20b3c3f0c53",
   "metadata": {},
   "source": [
    "Que3: How do you interpret the coefficients in a GLM?\n",
    "\n",
    "Ans: In this case you can interpret the coefficients as multiplying the probabilities by exp(β1) e x p ( β 1 ) , however these models can give you predicted probabilities greater than 1, and often don't converge (don't give an answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdba14-81b1-4afc-94d1-6386c494db0e",
   "metadata": {},
   "source": [
    "Que4: What is the difference between a univariate and multivariate GLM?\n",
    "\n",
    "Ans: univariate regression has one explanatory (predictor) variable x and multivariate regression has more at least two explanatory (predictor) variables x1,x2,...,xn . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8045a-c0d4-4eae-a2f4-cdc27320f8ec",
   "metadata": {},
   "source": [
    "Que5: Explain the concept of interaction effects in a GLM.\n",
    "\n",
    "Ans: Interaction effects include simultaneous effects of two or more variables on the process output or response. Interaction occurs when the effect of one independent variable changes depending on the level of another independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2046e25-aae5-4030-b286-d9ebe21dc219",
   "metadata": {},
   "source": [
    "Que6: How do you handle categorical predictors in a GLM?\n",
    "\n",
    "Ans: \n",
    "\n",
    "1) Drop Categorical Variables\n",
    "\n",
    "The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "2) Label Encoding\n",
    "\n",
    "Label encoding assigns each unique value to a different integer.\n",
    "\n",
    "3) One-Hot Encoding\n",
    "\n",
    "One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d1417-0abb-478c-b528-a54c2f301855",
   "metadata": {},
   "source": [
    "Que7: What is the purpose of the design matrix in a GLM?\n",
    "\n",
    "Ans: The purpose of the design matrix is to allow models that further constrain parameter sets. These constraints provide additional flexibility in modeling and allows researchers to build models that cannot be derived using the simple PIMs in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3186c-bfdf-47f7-9888-464bccfa2342",
   "metadata": {},
   "source": [
    "Que 8: How do you test the significance of predictors in a GLM?\n",
    "\n",
    "Ans: A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11473d61-1394-4508-96a4-b05838a85761",
   "metadata": {},
   "source": [
    "Que 9: What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d965a028-1316-409f-8940-f01be993c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef68adea-74de-44f6-ad91-588ed31a5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = ['sat', 'sat', 'sat', 'sat', 'sat', 'sat', 'sun', 'sun', 'sun', 'sun']\n",
    "weather = ['rain', 'rain', 'rain', 'rain', 'rain', 'sun', 'sun', 'sun', 'sun', 'sun']\n",
    "sales = [100, 100, 100, 100, 100, 10000, 10000, 10000, 10000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a2e50f-1f90-4de7-b51a-3847618f4cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday</th>\n",
       "      <th>weather</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sat</td>\n",
       "      <td>rain</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sat</td>\n",
       "      <td>rain</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sat</td>\n",
       "      <td>rain</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sat</td>\n",
       "      <td>rain</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sat</td>\n",
       "      <td>rain</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sat</td>\n",
       "      <td>sun</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sun</td>\n",
       "      <td>sun</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sun</td>\n",
       "      <td>sun</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sun</td>\n",
       "      <td>sun</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sun</td>\n",
       "      <td>sun</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weekday weather  sales\n",
       "0     sat    rain    100\n",
       "1     sat    rain    100\n",
       "2     sat    rain    100\n",
       "3     sat    rain    100\n",
       "4     sat    rain    100\n",
       "5     sat     sun  10000\n",
       "6     sun     sun  10000\n",
       "7     sun     sun  10000\n",
       "8     sun     sun  10000\n",
       "9     sun     sun  10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'weekday': weekday, 'weather': weather, 'sales': sales})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5630a1-ff2b-4c30-9c42-a8918c3f4e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        df        sum_sq       mean_sq             F  \\\n",
      "C(weekday)             1.0  1.633500e+08  1.633500e+08  2.438328e+31   \n",
      "C(weather)             1.0  8.167500e+07  8.167500e+07  1.219164e+31   \n",
      "C(weekday):C(weather)  1.0  1.464446e-24  1.464446e-24  2.185981e-01   \n",
      "Residual               7.0  4.689484e-23  6.699263e-24           NaN   \n",
      "\n",
      "                              PR(>F)  \n",
      "C(weekday)             3.689375e-108  \n",
      "C(weather)             4.174051e-107  \n",
      "C(weekday):C(weather)   6.543160e-01  \n",
      "Residual                         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Type I tells us that weekday is more important. The interaction effect is not signifcant.\n",
    "lm = ols('sales ~ C(weekday)*C(weather)',data=data).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=1) # Type 1 ANOVA DataFrame\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdfa018-eeb4-44ec-b9f8-3c8aacc7d559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  sum_sq   df             F         PR(>F)\n",
      "C(weekday)  1.654361e-23  1.0  2.509430e-01   6.317769e-01\n",
      "C(weather)  8.167500e+07  1.0  1.238893e+30  1.247833e-103\n",
      "Residual    4.614803e-22  7.0           NaN            NaN\n"
     ]
    }
   ],
   "source": [
    "# Type II tells us that weather is more important. There is no interaction effect.\n",
    "lm = ols('sales ~ C(weekday) + C(weather)',data=data).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2) # Type 2 ANOVA DataFrame\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7838885f-9aa1-49fa-8c9e-6d627f26bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             sum_sq   df             F         PR(>F)\n",
      "Intercept              5.000000e+04  1.0  7.463508e+27   7.353172e-96\n",
      "C(weekday)             1.118348e-22  1.0  1.669360e+01   4.655642e-03\n",
      "C(weather)             8.167500e+07  1.0  1.219164e+31  4.174051e-107\n",
      "C(weekday):C(weather)  2.382280e-23  1.0  3.556033e+00   1.013070e-01\n",
      "Residual               4.689484e-23  7.0           NaN            NaN\n"
     ]
    }
   ],
   "source": [
    "# Type III tells us that weekday is more important. The interaction effect is not signifcant.\n",
    "lm = ols('sales ~ C(weekday)*C(weather)',data=data).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=3) # Type 3 ANOVA DataFrame\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf696ebb-b991-4367-a13d-cd8688136ab4",
   "metadata": {},
   "source": [
    "10. Explain the concept of deviance in a GLM.\n",
    "\n",
    "Ans: Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It is defined as the difference between the Saturated and Proposed Models and can be thought as how much variation in the data does our Proposed Model account for. Therefore, the lower the deviance, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c31ac-db4c-4657-9a3a-b985c3ccba0f",
   "metadata": {},
   "source": [
    "## Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e18f2-004b-4bd8-a589-52661d9dbde3",
   "metadata": {},
   "source": [
    "Que 11: What is regression analysis and what is its purpose?\n",
    "\n",
    "Ans: Regression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables of interest. While there are many types of regression analysis, at their core they all examine the influence of one or more independent variables on a dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eaadc9-888a-411e-aa29-e5ee0347ca26",
   "metadata": {},
   "source": [
    "Que 12: What is the difference between simple linear regression and multiple linear regression?\n",
    "\n",
    "Ans: Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables. \n",
    "\n",
    "Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b85b12-f025-4069-b08f-5de0fdb9dec2",
   "metadata": {},
   "source": [
    "Que 13: How do you interpret the R-squared value in regression?\n",
    "\n",
    "Ans: In linear regression models, r squared interpretation is a goodness-fit-measure. It takes into account the strength of the relationship between the model and the dependent variable. Its convenience is measured on a scale of 0 – 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e19135-46fa-4875-92bf-6b91c62d69bd",
   "metadata": {},
   "source": [
    "Que 14: What is the difference between correlation and regression?\n",
    "\n",
    "Ans: Correlation is a statistical measure that determines the association or co-relationship between two variables.\n",
    "\n",
    "Regression describes how to numerically relate an independent variable to the dependent variable. To represent a linear relationship between two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a3f6c-0858-46be-9ae9-09e43ff7b0d9",
   "metadata": {},
   "source": [
    "Que 15: What is the difference between the coefficients and the intercept in regression?\n",
    "\n",
    "Ans: The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept; labeled as constant\n",
    "\n",
    "intercept is simply the expected value of Y at that value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bd40f-5deb-4d83-b415-7a464add42ad",
   "metadata": {},
   "source": [
    "Que 16: How do you handle outliers in regression analysis?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "Method 1: “Fogetaboutit…”\n",
    "\n",
    "    One option to dealing with outliers can be to drop the observations altogether. This can be a suitable option if it can be determined through further investigation that the survey entry was made in error. \n",
    "\n",
    "Method 2: Replacing The Outlier With a Another Value\n",
    "\n",
    "    If there is reason to believe that there could be reason to include outliers in the model, another option is to set a ceiling or floor for the variable in question.\n",
    "\n",
    "Method 3: Assign a Dummy Variable to Outliers\n",
    "\n",
    "    This is often my preferred option when dealing with outliers. It keeps allows the model to use all the sample data and also gives information about the outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d218af-aca0-476e-b11f-c34630d10b9a",
   "metadata": {},
   "source": [
    "Que 17: What is the difference between ridge regression and ordinary least squares regression?\n",
    "\n",
    "Ans: Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98631c1-7e07-4a58-8211-7fad6fe22616",
   "metadata": {},
   "source": [
    "Que 18: What is heteroscedasticity in regression and how does it affect the model?\n",
    "\n",
    "Ans: Heteroskedasticity refers to situations where the variance of the residuals is unequal over a range of measured values. When running a regression analysis, heteroskedasticity results in an unequal scatter of the residuals (also known as the error term)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c5f8e-3cc1-43ee-84b5-49f41b9bd29c",
   "metadata": {},
   "source": [
    "Que 19. How do you handle multicollinearity in regression analysis?\n",
    "\n",
    "Ans: Remove some of the highly correlated independent variables.\n",
    "\n",
    "Linearly combine the independent variables, such as adding them together.\n",
    "\n",
    "Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model.\n",
    "\n",
    "LASSO and Ridge regression are advanced forms of regression analysis that can handle multicollinearity. If you know how to perform linear least squares regression, you’ll be able to handle these analyses with just a little additional study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddf4f6-630d-4cbf-8aea-cc30424c1f1c",
   "metadata": {},
   "source": [
    "Que 20. What is polynomial regression and when is it used?\n",
    "\n",
    "Ans: A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44363078-b9ed-4ba4-a77b-994c6e0a51b1",
   "metadata": {},
   "source": [
    "## Loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56eea0-7bc1-4b23-b6ca-71e72f5f1b6b",
   "metadata": {},
   "source": [
    "Que 21. What is a loss function and what is its purpose in machine learning?\n",
    "\n",
    "Ans: a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d13e051-a1f5-428d-96bc-6b94a75d6332",
   "metadata": {},
   "source": [
    "Que 22. What is the difference between a convex and non-convex loss function?\n",
    "\n",
    "Ans: A convex function is one in which a line drawn between any two points on the graph lies on the graph or above it. There is only one requirement.\n",
    "\n",
    "A non-convex function is one in which a line drawn between any two points on the graph may cross additional points. It was described as “wavy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310f949-33f4-4f0d-b574-9f0b5c890fc9",
   "metadata": {},
   "source": [
    "Que 23. What is mean squared error (MSE) and how is it calculated?\n",
    "\n",
    "Ans: The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c464a4f-036e-4011-9b27-e6dc9b60bedb",
   "metadata": {},
   "source": [
    "Que 24. What is mean absolute error (MAE) and how is it calculated?\n",
    "\n",
    "Ans: The MAE score is measured as the average of the absolute error values. The Absolute is a mathematical function that makes a number positive. \n",
    "\n",
    "Mean Absolute Error (MAE) is calculated by taking the summation of the absolute difference between the actual and calculated values of each observation over the entire array and then dividing the sum obtained by the number of observations in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8607d-6634-4d12-9e0c-10bb41da58dc",
   "metadata": {},
   "source": [
    "Que 25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "\n",
    "Ans: Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.\n",
    "\n",
    "The cross entropy formula takes in two distributions, p(x)\n",
    ", the true distribution, and q(x)\n",
    ", the estimated distribution, defined over the discrete variable x\n",
    " and is given by\n",
    "\n",
    "H(p,q)=−∑∀xp(x)log(q(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326adb9-10e9-4595-a078-bf30649b1d9e",
   "metadata": {},
   "source": [
    "Que 26. How do you choose the appropriate loss function for a given problem?\n",
    "\n",
    "Ans: As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly. This requires the choice of an error function, conventionally called a loss function, that can be used to estimate the loss of the model so that the weights can be updated to reduce the loss on the next evaluation.\n",
    "\n",
    "Neural network models learn a mapping from inputs to outputs from examples and the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c9ce1-5c1e-483e-b371-6eadb84632f1",
   "metadata": {},
   "source": [
    "Que 27. Explain the concept of regularization in the context of loss functions.\n",
    "\n",
    "Ans: During the L2 regularization the loss function of the neural network as extended by a so-called regularization term, which is called here Ω. The regularization term Ω is defined as the Euclidean Norm (or L2 norm) of the weight matrices, which is the sum over all squared weight values of a weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c3f38-4199-4a35-9d2a-edee6c320afc",
   "metadata": {},
   "source": [
    "Que 28. What is Huber loss and how does it handle outliers?\n",
    "\n",
    "Ans: In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. \n",
    "\n",
    "Median is much more robust to outliers than mean. Huber loss is a balanced compromise between these two types. It is robust to the outliers but does not completely ignore them either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bdd55e-7575-4dd0-84d1-ff35916976d9",
   "metadata": {},
   "source": [
    "Que 29. What is quantile loss and when is it used?\n",
    "\n",
    "Ans: quantile loss — a flexible loss function that can be incorporated into any regression model to predict a certain variable quantile. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801113f-15a9-4831-b2f2-29210169fef9",
   "metadata": {},
   "source": [
    "Que 30. What is the difference between squared loss and absolute loss?\n",
    "\n",
    "Ans: For square loss, you will choose the estimated mean of y0, as the true mean minimizes square loss on average (where the average is taken across random samples of y0 subject to x=x0).\n",
    "\n",
    "For absolute loss, you will choose the estimated median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ce835-32f5-437f-a491-d3e9eb64d78e",
   "metadata": {},
   "source": [
    "## Optimizer (GD):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467819b8-5496-46c5-a47d-bc359e8c42ae",
   "metadata": {},
   "source": [
    "Que 31. What is an optimizer and what is its purpose in machine learning?\n",
    "\n",
    "Ans: optimization is a process of finding optimal parameters for the model, which significantly reduces the error function.\n",
    "\n",
    "The ultimate goal of ML model is to reach the minimum of the loss function. After we pass input, we calculate the error and update the weights accordingly. This is where optimizer comes into play. It defines how to tweak the parameters to get closer to the minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80faaee-f0e8-4567-8198-4f6688b26d3b",
   "metadata": {},
   "source": [
    "Que 32. What is Gradient Descent (GD) and how does it work?\n",
    "\n",
    "Ans: Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eeb7e-4e68-45d0-a9cc-ce525e3eeff9",
   "metadata": {},
   "source": [
    "Que 33. What are the different variations of Gradient Descent?\n",
    "\n",
    "Ans: Three simple variants of gradient descent algorithms, namely batch gradient descent, stochastic gradient descent and mini-batch gradient descent are compared in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc29801-6109-427e-a6e1-0ee1c652b3e4",
   "metadata": {},
   "source": [
    "Que 34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "\n",
    "Ans: The learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated.\n",
    "\n",
    "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68bf67-57b9-4bca-bbee-283b0de75dfb",
   "metadata": {},
   "source": [
    "Que 35. How does GD handle local optima in optimization problems?\n",
    "\n",
    "Ans: A local optima is the extrema (minimum or maximum) of the objective function for a given region of the input space, e.g. a basin in a minimization problem.\n",
    "\n",
    "Gradient Descent is an iterative process that finds the minima of a function. This is an optimisation algorithm that finds the parameters or coefficients of a function where the function has a minimum value. Although this function does not always guarantee to find a global minimum and can get stuck at a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d75f5-556e-44a6-93f9-7ed24b573c6c",
   "metadata": {},
   "source": [
    "Que 36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "\n",
    "Ans: SGD tries to solve the main problem in Batch Gradient descent which is the usage of whole training data to calculate gradients at each step. SGD is stochastic in nature i.e. it picks up a “random” instance of training data at each step and then computes the gradient, making it much faster as there is much fewer data to manipulate at a single time, unlike Batch GD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e73d9-e1f5-465b-a56d-cb7738aab946",
   "metadata": {},
   "source": [
    "Que 37. Explain the concept of batch size in GD and its impact on training.\n",
    "\n",
    "Ans: The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d28360-6bc4-4a2e-80c4-cc2987358a8b",
   "metadata": {},
   "source": [
    "Que 38. What is the role of momentum in optimization algorithms?\n",
    "\n",
    "Ans: Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search space and overcome the oscillations of noisy gradients and coast across flat spots of the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345883bd-a016-4c86-9a95-1c821a03ce58",
   "metadata": {},
   "source": [
    "Que 39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "\n",
    "Ans: \n",
    "\n",
    "* Batch Gradient Descent:\n",
    "\n",
    "The samples from the whole dataset are used to optimize the parameters i.e to compute the gradients for a single update. For a dataset of 100 samples, updates occur only once.\n",
    "\n",
    "* Mini Batch Gradient Descent:\n",
    "\n",
    "This is meant to capture the good aspects of Batch and Stochastic GD. Instead of a single sample ( Stochastic GD ) or the whole dataset ( Batch GD ), we take small batches or chunks of the dataset and update the parameters accordingly. For a dataset of 100 samples, if the batch size is 5 meaning we have 20 batches. Hence, updates occur 20 times.\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "Stochastic GD computes the gradients for each and every sample in the dataset and hence makes an update for every sample in the dataset. For a dataset of 100 samples, updates occur 100 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a0863-04bd-4f60-99d9-4232539cd87c",
   "metadata": {},
   "source": [
    "Que 40. How does the learning rate affect the convergence of GD?\n",
    "\n",
    "Ans:  the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n",
    "\n",
    "The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
    "\n",
    "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31850f7-207e-41f3-873a-810b2afb4377",
   "metadata": {},
   "source": [
    "## Regularization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612fb3c-f3a4-42a7-bc12-32b688673137",
   "metadata": {},
   "source": [
    "Que 41. What is regularization and why is it used in machine learning?\n",
    "\n",
    "Ans: Regularization is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting by adding extra information to it.\n",
    "\n",
    "This technique can be used in such a way that it will allow to maintain all variables or features in the model by reducing the magnitude of the variables. Hence, it maintains accuracy as well as a generalization of the model.\n",
    "\n",
    "It mainly regularizes or reduces the coefficient of features toward zero. In simple words, \"In regularization technique, we reduce the magnitude of the features by keeping the same number of features.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770971b-c562-4396-855c-4290b2567b84",
   "metadata": {},
   "source": [
    "Que 42: What is the difference between L1 and L2 regularization?\n",
    "\n",
    "Ans: L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc5637-7cd4-43b8-934c-751d43b87fd1",
   "metadata": {},
   "source": [
    "Que 43. Explain the concept of ridge regression and its role in regularization.\n",
    "\n",
    "Ans: Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions. Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339965d-1b9b-4307-95c4-0628a03d2961",
   "metadata": {},
   "source": [
    "Que 44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "\n",
    "Ans: The elastic net is a linear regression regularization technique that combines both the L1 (Lasso) and L2 (Ridge) regularization penalties. It is particularly useful when dealing with datasets that have high collinearity or when there are more predictors than observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f05661-b996-453b-b5a3-b51ce7331c49",
   "metadata": {},
   "source": [
    "Que 45. How does regularization help prevent overfitting in machine learning models?\n",
    "\n",
    "Ans: Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504ed3f-1019-4151-8b13-84f0b4894590",
   "metadata": {},
   "source": [
    "Que 46. What is early stopping and how does it relate to regularization?\n",
    "\n",
    "Ans: In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdc351-5eb7-461e-9041-a8bae6e7ca23",
   "metadata": {},
   "source": [
    "Que 47. Explain the concept of dropout regularization in neural networks.\n",
    "\n",
    "Ans: Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae61b5-8f5b-4149-b3d1-11f86d696665",
   "metadata": {},
   "source": [
    "Que 48. How do you choose the regularization parameter in a model?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "1) on the training set, we estimate several different Ridge regressions, with different values of the regularization parameter;\n",
    "\n",
    "2) on the validation set, we choose the best model (the regularization parameter which gives the lowest MSE on the validation set);\n",
    "\n",
    "3) on the test set, we check how much overfitting we have done by doing model selection on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7950d-7fff-4cb0-8808-cf078eb99251",
   "metadata": {},
   "source": [
    "Que 49. What is the difference between feature selection and regularization?\n",
    "\n",
    "Ans: Feature selection, also known as feature subset selection, variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference. \n",
    "\n",
    "Regularization, where we are constraining the solution space while doing optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a564b-d89c-496d-939e-2b729bc1cb3e",
   "metadata": {},
   "source": [
    "Que 50. What is the trade-off between bias and variance in regularized models?\n",
    "\n",
    "Ans: If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68153d1-2166-4ea6-9f9c-a8fd1188d0f1",
   "metadata": {},
   "source": [
    "## SVM:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773c64e-d903-4d7e-9fa2-8465558202a1",
   "metadata": {},
   "source": [
    "Que 51. What is Support Vector Machines (SVM) and how does it work?\n",
    "\n",
    "Ans: Support vector machines (SVMs) are powerful machine learning tools for data classification and prediction (Vapnik, 1995). The problem of separating two classes is handled using a hyperplane that maximizes the margin between the classes (Fig. 8.8). The data points that lie on the margins are called support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1e602-ffa8-4ab8-9eb9-4588ac5c9fb0",
   "metadata": {},
   "source": [
    "Que 52. How does the kernel trick work in SVM?\n",
    "\n",
    "Ans: Kernel trick allows the inner product of mapping function instead of the data points. The trick is to identify the kernel functions which can be represented in place of the inner product of mapping functions. Kernel functions allow easy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f12fa-4679-49b0-9290-e37de13d973a",
   "metadata": {},
   "source": [
    "Que 53. What are support vectors in SVM and why are they important?\n",
    "\n",
    "Ans: Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc013ccc-29d4-4d21-b034-5b717e9f3e48",
   "metadata": {},
   "source": [
    "Que 54. Explain the concept of the margin in SVM and its impact on model performance.\n",
    "\n",
    "Ans: Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin.\n",
    "\n",
    "The performance of the SVM depends on different parameters such as penalty factor, , and the kernel factor, . Also choosing an appropriate kernel function can improve the recognition score and lower the amount of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679bcba-d066-4af3-9ff6-a40fb90ce34b",
   "metadata": {},
   "source": [
    "Que 55. How do you handle unbalanced datasets in SVM?\n",
    "\n",
    "Ans: Perhaps the simplest and most common extension to SVM for imbalanced classification is to weight the C value in proportion to the importance of each class. To accommodate these factors in SVMs an instance-level weighted modification was proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c59ebb-34df-4c8d-a44c-70e166562c9e",
   "metadata": {},
   "source": [
    "Que 56. What is the difference between linear SVM and non-linear SVM?\n",
    "\n",
    "Ans: Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49420aa-e6af-4fe3-a6ff-16b69dde10d9",
   "metadata": {},
   "source": [
    "Que 57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "\n",
    "Ans: C parameter adds a penalty for each misclassified data point. If c is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee702b9b-7917-4430-8a6f-89cb47ec2527",
   "metadata": {},
   "source": [
    "Que 58. Explain the concept of slack variables in SVM.\n",
    "\n",
    "Ans: Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea155e-011e-4830-9d52-cd541d7252a9",
   "metadata": {},
   "source": [
    "Que 59. What is the difference between hard margin and soft margin in SVM?\n",
    "\n",
    "Ans: When the data is linearly separable, and we don't want to have any misclassifications, we use SVM with a hard margin. However, when a linear boundary is not feasible, or we want to allow some misclassifications in the hope of achieving better generality, we can opt for a soft margin for our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ecae9-40cc-4ab5-a6cd-ea1378db6c56",
   "metadata": {},
   "source": [
    "Que 60. How do you interpret the coefficients in an SVM model?\n",
    "\n",
    "Ans: Recall that in linear SVM, the result is a hyperplane that separates the classes as best as possible. The weights represent this hyperplane, by giving you the coordinates of a vector which is orthogonal to the hyperplane - these are the coefficients given by svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82815a-bf87-4c26-bf71-3df16c903fe3",
   "metadata": {},
   "source": [
    "## Decision Trees:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38829e90-312e-41f9-8b1c-23fb8490b603",
   "metadata": {},
   "source": [
    "Que 61. What is a decision tree and how does it work?\n",
    "\n",
    "Ans:  A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
    "\n",
    "* Steps :\n",
    "\n",
    "1) Start with your idea. Begin your diagram with one main idea or decision. ...\n",
    "\n",
    "2) Add chance and decision nodes. ...\n",
    "\n",
    "3) Expand until you reach end points. ...\n",
    "\n",
    "4) Calculate tree values. ...\n",
    "\n",
    "5) Evaluate outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00470a7-60ca-4942-becf-c5fb0115c53c",
   "metadata": {},
   "source": [
    "Que 62. How do you make splits in a decision tree?\n",
    "\n",
    "Ans: For each split, individually calculate the entropy of each child node. Calculate the entropy of each split as the weighted average entropy of child nodes. Select the split with the lowest entropy or highest information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de419e7b-8f08-4dc1-a806-c17cab0e17c9",
   "metadata": {},
   "source": [
    "Que 63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "\n",
    "Ans: The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits. To put it into context, a decision tree is trying to create sequential questions such that it partitions the data into smaller groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa572f-0a6d-466e-8d0e-a3c883114a8e",
   "metadata": {},
   "source": [
    "Que 64. Explain the concept of information gain in decision trees.\n",
    "\n",
    "Ans: Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9256bd-7181-4918-b9dd-8d637a8d476b",
   "metadata": {},
   "source": [
    "Que 65. How do you handle missing values in decision trees?\n",
    "\n",
    "Ans: It will consider the missing values by taking the majority of the K nearest values. The random forest also is robust to categorical data with missing values. Many decision tree-based algorithms like XGBoost, Catboost support data with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c41b4-2666-45fd-a009-1bc0f7afb4e8",
   "metadata": {},
   "source": [
    "Que 66. What is pruning in decision trees and why is it important?\n",
    "\n",
    "Ans: A Decision tree that is trained to its full depth will highly likely lead to overfitting the training data - therefore Pruning is important. In simpler terms, the aim of Decision Tree Pruning is to construct an algorithm that will perform worse on training data but will generalize better on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71275e5d-4ee1-4809-b228-248bec080441",
   "metadata": {},
   "source": [
    "Que 67. What is the difference between a classification tree and a regression tree?\n",
    "\n",
    "Ans: Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d57040-7405-41dd-a9c6-89da32a886a7",
   "metadata": {},
   "source": [
    "Que 68. How do you interpret the decision boundaries in a decision tree?\n",
    "\n",
    "Ans : Decision boundary of a decision tree is determined by overlapping orthogonal half-planes (representing the result of each subsequent decision) and can end up as displayed on the pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ccd43-affb-4022-adde-16a158575ff7",
   "metadata": {},
   "source": [
    "Que 69. What is the role of feature importance in decision trees?\n",
    "\n",
    "Ans: Feature importance is a common way to make interpretable machine learning models and also explain existing models. That enables to see the big picture while taking decisions and avoid black box models. We've mentioned feature importance for linear regression and decision trees before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4a07f-0e44-4c23-8060-0300ba54c7f9",
   "metadata": {},
   "source": [
    "Que 70. What are ensemble techniques and how are they related to decision trees?\n",
    "\n",
    "Ans: Using one decision tree is can be problematic and might not be stable enough; however, using multiple decision trees and combining their results will do great. Combining multiple classifiers in a prediction model is called ensembling. The simple rule of ensemble methods is to reduce the error by reducing the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ffa3ae-1c1e-47df-8363-ac0bb09f3d43",
   "metadata": {},
   "source": [
    "## Ensemble Techniques:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf247c-e0c0-4196-be40-c8a6198d1d7f",
   "metadata": {},
   "source": [
    "Que 71. What are ensemble techniques in machine learning?\n",
    "\n",
    "Ans: Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods in machine learning usually produce more accurate solutions than a single model would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e1336-fb35-4c7a-8999-67990c286a57",
   "metadata": {},
   "source": [
    "Que 72. What is bagging and how is it used in ensemble learning?\n",
    "\n",
    "Ans: Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8dc5b-cbcb-4197-ab2f-be69e7f600d6",
   "metadata": {},
   "source": [
    "Que 73. Explain the concept of bootstrapping in bagging.\n",
    "\n",
    "Ans: Bagging is composed of two parts: aggregation and bootstrapping. Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method. The learning algorithm is then run on the samples selected.\n",
    "\n",
    "The bootstrapping technique uses sampling with replacements to make the selection procedure completely random. When a sample is selected without replacement, the subsequent selections of variables are always dependent on the previous selections, making the criteria non-random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd0cd8-d081-4546-879f-3355bd1c8871",
   "metadata": {},
   "source": [
    "Que 74. What is boosting and how does it work?\n",
    "\n",
    "Ans: Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data. A single machine learning model might make prediction errors depending on the accuracy of the training dataset. For example, if a cat-identifying model has been trained only on images of white cats, it may occasionally misidentify a black cat. Boosting tries to overcome this issue by training multiple models sequentially to improve the accuracy of the overall system.\n",
    "\n",
    "Decision trees\n",
    "Decision trees are data structures in machine learning that work by dividing the dataset into smaller and smaller subsets based on their features. The idea is that decision trees split up the data repeatedly until there is only one class left. For example, the tree may ask a series of yes or no questions and divide the data into categories at every step.\n",
    "\n",
    "Boosting ensemble method\n",
    "Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree. After numerous cycles, the boosting method combines these weak rules into a single powerful prediction rule.\n",
    "\n",
    "Boosting compared to bagging\n",
    "Boosting and bagging are the two common ensemble methods that improve prediction accuracy. The main difference between these learning methods is the method of training. In bagging, data scientists improve the accuracy of weak learners by training several of them at once on multiple datasets. In contrast, boosting trains weak learners one after another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae3b5f-eaae-4544-aea1-017615b43174",
   "metadata": {},
   "source": [
    "Que 75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "\n",
    "Ans: AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f98078-b711-4688-a296-a34754180463",
   "metadata": {},
   "source": [
    "Que 76. What is the purpose of random forests in ensemble learning?\n",
    "\n",
    "Ans: A random forest is a machine learning technique that's used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18284ac-faf3-437c-bc71-fc478745863e",
   "metadata": {},
   "source": [
    "Que 77. How do random forests handle feature importance?\n",
    "\n",
    "Ans: The final feature importance, at the Random Forest level, is it's average over all the trees. The sum of the feature's importance value on each trees is calculated and divided by the total number of trees: RFfi sub(i)= the importance of feature i calculated from all trees in the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5be925-d25e-4d0b-8c90-67e48a5b4c3d",
   "metadata": {},
   "source": [
    "Que 78. What is stacking in ensemble learning and how does it work?\n",
    "\n",
    "Ans: \n",
    "Stacking in Machine Learning - Javatpoint\n",
    "Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance.\n",
    "\n",
    "Stacking involves training multiple base-models to predict the target variable in a machine learning problem while at the same time, a meta-model learns to use the predictions of each base model to predict the value of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620268a2-b427-4b35-a864-e0b322ac0404",
   "metadata": {},
   "source": [
    "Que 79. What are the advantages and disadvantages of ensemble techniques?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Pros:\n",
    "    \n",
    "    Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data. Furthermore, ensemble methods can handle different types of data and tasks, such as classification, regression, clustering, and anomaly detection, by using different types of base models and aggregation methods. Additionally, they can provide more confidence and reliability by measuring the diversity and agreement of the base models, and by providing confidence intervals and error estimates for the predictions.\n",
    "    \n",
    "Cons:\n",
    "\n",
    "    Ensemble methods have some drawbacks and challenges, such as being computationally expensive and time-consuming due to the need for training and storing multiple models, and combining their outputs. This can increase the complexity and memory requirements of the system. Additionally, they can be difficult to interpret and explain, as they involve multiple layers of abstraction and aggregation, which can obscure the logic and reasoning behind the predictions. Furthermore, they can be prone to overfitting and underfitting if the base models are too weak or too strong, or if the aggregation method is too simple or too complex. This can lead to underestimating or overestimating the uncertainty and variability of the data. Lastly, they can be sensitive to the quality and diversity of the data and the base models, as they depend on the assumptions and limitations of the individual models, and on the representativeness and independence of the data samples and features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d30e1-4d76-4434-a93a-af92848faca7",
   "metadata": {},
   "source": [
    "Que 80. How do you choose the optimal number of models in an ensemble?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Step 1 : Find the KS of individual models. \n",
    "\n",
    "Step 2: Index all the models for easy access. \n",
    "\n",
    "Step 3: Choose the first two models as the initial selection and set a correlation limit. \n",
    "\n",
    "Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
