{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0baab35-01d5-4d56-a46f-7538f5fb1689",
   "metadata": {},
   "source": [
    "Que 1. How do word embeddings capture semantic meaning in text preprocessing?\n",
    "\n",
    "Ans: Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions.\n",
    "\n",
    "Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these representations have been criticized for lacking interpretable dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f7e2f-0739-4b77-9a81-92243b31f7b4",
   "metadata": {},
   "source": [
    "Que 2. Explain the concept of recurrent neural networks (RNNs) and their role in text processing tasks.\n",
    "\n",
    "Ans: Recurrent Neural Networks (RNNs) are a form of machine learning algorithm that are ideal for sequential data such as text, time series, financial data, speech, audio, video among others. RNNs are ideal for solving problems where the sequence is more important than the individual items themselves.\n",
    "\n",
    "RNN's perform very well for applications where sequential information is clearly important, because the meaning could be misinterpreted or the grammar could be incorrect if sequential information is not used. Applications include image captioning, language modeling and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bcb5f-1e6e-4253-81ae-2b796c61dd74",
   "metadata": {},
   "source": [
    "Que 3. What is the encoder-decoder concept, and how is it applied in tasks like machine translation or text summarization?\n",
    "\n",
    "Ans: The encoder-decoder model is a way of using recurrent neural networks for sequence-to-sequence prediction problems. It was initially developed for machine translation problems, although it has proven successful at related sequence-to-sequence prediction problems such as text summarization and question answering.\n",
    "\n",
    "The encoder is at the feeding end; it understands the sequence and reduces the dimension of the input sequence. The sequence has a fixed size known as the context vector. This context vector acts like input to the decoder, which generates an output sequence when reaching the end token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d5e99-0549-479b-955a-51099ef6a39c",
   "metadata": {},
   "source": [
    "Que 4. Discuss the advantages of attention-based mechanisms in text processing models.\n",
    "\n",
    "Ans: The attention mechanism allows the model to \"pay attention\" to certain parts of the data and to give them more weight when making predictions. In a nutshell, the attention mechanism helps preserve the context of every word in a sentence by assigning an attention weight relative to all other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c33ea4-817f-4fe6-b8ed-4dc443c29de2",
   "metadata": {},
   "source": [
    "Que 5. Explain the concept of self-attention mechanism and its advantages in natural language processing.\n",
    "\n",
    "Ans: \n",
    "The self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n",
    "\n",
    "In Self-Attention or K=V=Q, if the input is, for example, a sentence, then each word in the sentence needs to undergo Attention computation. The goal is to learn the dependencies between the words in the sentence and use that information to capture the internal structure of the sentence.\n",
    "\n",
    "* Advantages of self attention:\n",
    "\n",
    "Minimize total computational complexity per layer. Maximize amount of parallelizable computations, measured by minimum number of sequential operations required. Minimize maximum path length between any two input and output positions in network composed of the different layer types ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c233b-05ef-4896-92c0-86a77d288829",
   "metadata": {},
   "source": [
    "Que 6. What is the transformer architecture, and how does it improve upon traditional RNN-based models in text processing?\n",
    "\n",
    "Ans: The Transformer architecture uses an encoder-decoder structure that does not rely on recurrence and convolutions to generate an output. The encoder maps an input sequence to a series of continuous representations.\n",
    "\n",
    "\n",
    "Transformers have several advantages over RNNs. They excel in capturing global contextual information, exhibiting superior performance in tasks that require long-term dependencies.\n",
    "\n",
    "Unlike RNNs, transformers do not have a recurrent structure. Provided with enough training data, their attention mechanisms alone can match the performance of RNNs with attention added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b076f6de-e066-40c6-b166-171467ae6304",
   "metadata": {},
   "source": [
    "Que 7. Describe the process of text generation using generative-based approaches.\n",
    "\n",
    "Ans: Text generation is a subfield of natural language processing (NLP) that deals with generating text automatically. It has a wide range of applications, including machine translation, content creation, and conversational agents.\n",
    "\n",
    "A text-to-text Generative AI is an AI that Generates text based on text input. An example of a text-to-text Generative AI is ChatGPT, developed by OpenAI. Text generation uses machine learning, existing data and previous user input in generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ff97d-c83e-4169-9fa2-a77c12834826",
   "metadata": {},
   "source": [
    "Que 8. What are some applications of generative-based approaches in text processing?\n",
    "\n",
    "Ans: Potential applications: Data augmentation, dataset synthesis, art creation, code generation, text generation, audio synthesis, etc. Synthesized by score-based generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06151bd7-3cdb-44b7-b83e-e80a7e6fc891",
   "metadata": {},
   "source": [
    "Que 9. Discuss the challenges and techniques involved in building conversation AI systems.\n",
    "\n",
    "Ans: Techniques involved in building conversation AI systems.\n",
    "\n",
    "1. Rule-Based\n",
    "Rule-based systems are trained on a predefined hierarchy of rules that govern how to transform user input into output dialogue or actions. Rules can range from simple to complex, and a rule-based system is relatively straightforward to create. However, these systems aren’t able to respond to input patterns or keywords that don’t match existing rules.\n",
    "\n",
    "2. Retrieval-Based\n",
    "Retrieval-based methods power the bulk of production systems in use today.\n",
    "When given user input, the system uses heuristics to locate the best response from its database of pre-defined responses. Dialogue selection is essentially a prediction problem, and using heuristics to identify the most appropriate response template may involve simple algorithms like keywords matching or it may require more complex processing with machine learning or deep learning. \n",
    "\n",
    "3. Generative Methods\n",
    "Overcoming the limitations of the previous two approaches requires that the conversational AI be smart enough and creative enough to generate new content. Instead of drawing upon pre-defined responses, conversational AI that use generative methods is given a large amount of conversational training data in order to learn how to generate new dialogue that resembles it.\n",
    "\n",
    "4. Ensemble Methods\n",
    "Recent, state-of-the-art conversational AI such as Alexa prize bots, which were designed to be conversational bots that could talk about any subject (a very difficult problem!), have been built with ensemble methods, which use some combination of rule-based, retrieval-based, and generative method approaches as dictated by context.\n",
    "\n",
    "5. Grounded Learning\n",
    "Human dialogue relies extensively on context and external knowledge. For example, if you told a chatbot that you were going to the Swan Oyster Depot, that chatbot would probably recognize Swan Oyster Depot as a restaurant, possibly a seafood restaurant, and it may tell you to have a good time. \n",
    "\n",
    "6. Interactive Learning\n",
    "Language is inherently interactive. Humans use language to facilitate cooperation when they needed to solve problem together, and practical needs influences how language continues to develop.\n",
    "For conversational AI, interactive learning remains an area of active study despite decades of continued development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40833865-89b8-4634-bdf0-d88641563586",
   "metadata": {},
   "source": [
    "Que 10. How do you handle dialogue context and maintain coherence in conversation AI models?\n",
    "\n",
    "Ans: There are 5 broad approaches to managing a conversation:\n",
    "\n",
    "1) Intents, Entities, Dialog State Systems & Bot Messages\n",
    "\n",
    "This is the stock-standard approach to digital assistants or chatbots. Intents at the frontline, sometimes fronted by a high-pass Natural Language Processing (NLP) layer for sentence boundary detection, summarisation, classification, keywords extraction, etc\n",
    "\n",
    "2) Machine Learning Stories\n",
    "\n",
    "The ideal would be to have a probabilistic classifier of sorts, observing the user input and replying with the most appropriate message. And where a rigid set of steps are required, like opening an account, a sequential set-states approach can be followed. Rasa, with their ML Stories also have a Rules approach. This is a type of training used to train where short pieces of conversations are described, which should always follow the same sequence.\n",
    "\n",
    "3) Chitchat / Smalltalk\n",
    "\n",
    "Many chatbot implementations cater for chitchat implementations.\n",
    "This is were smalltalk is incorporated into the chatbot, which includes basic curtesy. And the handling of errors and exceptions gracefully and in a highly conversational manner.\n",
    "\n",
    "4) Large Language Models (LLM)\n",
    "\n",
    "Large Language Models (LLM) have a whole array of implementations with which the dialog of a conversational agent can be created. LLM’s give a large degree of flexibility, with zero to few shot training. In other words, much can be achieved with no to very little training data or effort. This flexibility is astounding at first and the implementation possibilities flood one’s mind.\n",
    "\n",
    "5) Knowledge Base Management\n",
    "\n",
    "Firstly, one could address Questions and Answering via alternative approach, other than traditional Knowledge Bases. Doing so via traditional chatbot development affordances; intents, entities, dialog trees and response messages.\n",
    "\n",
    "6) Second level\n",
    "\n",
    "A second level is where a custom knowledge base is setup. This can be done via various means, Elasticsearch, Watson Discovery, Rasa knowledge base actions, OpenAI Language API with fine-tuning or Pinecone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5909a-09e5-46b3-a692-6ea9973290a8",
   "metadata": {},
   "source": [
    "Que 11. Explain the concept of intent recognition in the context of conversation AI.\n",
    "\n",
    "Ans: Intent recognition is the process of identifying and understanding a user's intention or goal behind a given text or speech input in a conversational AI system.\n",
    "\n",
    "The purpose of intent classification is to analyze and then group the messages into “intents” that represent the information the user is looking for. Intent classification is a key aspect of the chatbot development process and it makes the bot capable of offering responses in a natural way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bb9d3-96c6-435b-a035-29fdcf6285a7",
   "metadata": {},
   "source": [
    "Que 12. Discuss the advantages of using word embeddings in text preprocessing.\n",
    "\n",
    "Ans: Word Embeddings help us understand the meaning of each word, which can be used to recommend articles, suggest automations, and enable more features based on the dialogue meaning.\n",
    "\n",
    "Represent words as semantically-meaningful dense real-valued vectors. This overcomes many of the problems that simple one-hot vector encodings have. Most importantly, embeddings boost generalisation and performance for pretty much any NLP problem, especially if you don't have a lot of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0f3b1-39d7-46c1-9202-a69f2dc29e9d",
   "metadata": {},
   "source": [
    "Que 13. How do RNN-based techniques handle sequential information in text processing tasks?\n",
    "\n",
    "Ans: RNN were created because there were a few issues in the feed-forward neural network:\n",
    "\n",
    "1) Cannot handle sequential data\n",
    "\n",
    "2) Considers only the current input\n",
    "\n",
    "3) Cannot memorize previous inputs\n",
    "\n",
    "The solution to these issues is the RNN. An RNN can handle sequential data, accepting the current input data, and previously received inputs. RNNs can memorize previous inputs due to their internal memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a538ea-6fe8-4780-bfa9-6d9208f0a5d8",
   "metadata": {},
   "source": [
    "Que 14. What is the role of the encoder in the encoder-decoder architecture?\n",
    "\n",
    "Ans: The architecture of Encoder-Decoder -\n",
    "It consists of 3 parts: encoder, intermediate vector and decoder.\n",
    "\n",
    "Encoder-It accepts a single element of the input sequence at each time step, process it, collects information for that element and propagates it forward.\n",
    "\n",
    "* The encoder is basically LSTM/GRU cell.\n",
    "\n",
    "* An encoder takes the input sequence and encapsulates the information as the internal state vectors.\n",
    "\n",
    "* Outputs of the encoder are rejected and only internal states are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933febb-fb13-499f-8856-546d6faa3dfe",
   "metadata": {},
   "source": [
    "Que 15. Explain the concept of attention-based mechanism and its significance in text processing.\n",
    "\n",
    "Ans: The attention mechanism allows the model to \"pay attention\" to certain parts of the data and to give them more weight when making predictions. In a nutshell, the attention mechanism helps preserve the context of every word in a sentence by assigning an attention weight relative to all other words.\n",
    "\n",
    "The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14deff5-eb65-48cf-9903-d37da657583f",
   "metadata": {},
   "source": [
    "Que 16. How does self-attention mechanism capture dependencies between words in a text?\n",
    "\n",
    "Ans:In Self-Attention or K=V=Q, if the input is, for example, a sentence, then each word in the sentence needs to undergo Attention computation. The goal is to learn the dependencies between the words in the sentence and use that information to capture the internal structure of the sentence.\n",
    "\n",
    "As for a reason behind using Self-Attention mechanisms, the paper brings up three main points (complexity of each layer, run-ability, distant dependency learning) and gives comparisons to the complexity of RNN and CNN computations.\n",
    "\n",
    "We can see that if the input sequence n is smaller than the representative dimension d, then Self-Attention is advantageous regarding the time complexity of each layer.\n",
    "\n",
    "When n is larger, the author provides a solution in Self-Attention (restricted), where not every word undergoes Attention calculation, instead only r words undergo the calculation.\n",
    "\n",
    "Regarding concurrency, Multi-Head Attention and CNN both depend on calculations from the previous instance and features better concurrency than RNN.\n",
    "\n",
    "In distant dependencies, since self-attention is applied to both each word and all words together, no matter how distant they are, the longest possible path is one so that the system can capture distant dependency relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d06cc3-f0c1-49da-a9ce-559ad447eda6",
   "metadata": {},
   "source": [
    "Que 17. Discuss the advantages of the transformer architecture over traditional RNN-based models\n",
    "\n",
    "Ans: Transformers (Attention is all you need) were introduced in the context of machine translation with the purpose to avoid recursion in order to allow parallel computation (to reduce training time) and also to reduce drops in performance due to long dependencies. The main characteristics are:\n",
    "\n",
    "1) Non sequential: sentences are processed as a whole rather than word by word.\n",
    "\n",
    "2) Self Attention: this is the newly introduced 'unit' used to compute similarity scores between words in a sentence.\n",
    "\n",
    "3) Positional embeddings: another innovation introduced to replace recurrence. The idea is to use fixed or learned weights which encode information related to a specific position of a token in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19352212-3227-4ce3-b67b-746f3b5eef33",
   "metadata": {},
   "source": [
    "Que 18. What are some applications of text generation using generative-based approaches?\n",
    "\n",
    "Ans: \n",
    "\n",
    "1) One of the AI models that can generate text is GPT (Generative Pre-trained Transformer), or generative pre-trained transformer. This language model, built by OpenAI and released in 2020, has different models, including GPT-3.\n",
    "\n",
    "2) GPT-3 is a much larger model than its predecessor, with over 175 billion parameters. It is trained on a variety of data sources, including books, articles, and code repositories to generate realistic texts like human writers. It is possible to create summaries, answer questions, use as a grammar checker, learn new ideas and make translations through GPT-3.   \n",
    "\n",
    "3) Another approach to text generation is to use a template-based model. 4  Unlike GPT-3, these models do not work independently, and intermediate steps require human intervention. It is possible, however, to produce more structured texts based on templates without requiring humans to edit and control them after they are generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdabb8-84a1-4616-aafc-320c36e1f5dd",
   "metadata": {},
   "source": [
    "Que 20. Explain the concept of natural language understanding (NLU) in the context of conversation AI.\n",
    "\n",
    "Ans: NLU enables human-computer interaction. It is the comprehension of human language such as English, Spanish and French, for example, that allows computers to understand commands without the formalized syntax of computer languages. NLU also enables computers to communicate back to humans in their own languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084e6dd-34e9-4c0a-b12a-c486f23223b8",
   "metadata": {},
   "source": [
    "Que 21. What are some challenges in building conversation AI systems for different languages or domains?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "1) Regional jargon and slang.\n",
    "\n",
    "2) Dialects not conforming to standard language.\n",
    "\n",
    "3) Background noise distorting the voice of the speaker.\n",
    "\n",
    "4) Unscripted questions that the virtual assistant or chatbot does not know to answer.\n",
    "\n",
    "5) Unplanned responses by customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f8abe-5e97-47c4-96b4-8fa60f044a68",
   "metadata": {},
   "source": [
    "Que 22. Discuss the role of word embeddings in sentiment analysis tasks.\n",
    "\n",
    "Ans: Word embeddings are dense vectors with much lower dimensionality. Secondly, the semantic relationships between words are reflected in the distance and direction of the vectors.\n",
    "\n",
    "Word embeddings are computed differently. Each word is positioned into a multi-dimensional space. The number of dimensions in this space is chosen by the data scientist. You can experiment with different dimensions and see what provides the best result.\n",
    "\n",
    "The vector values for a word represent its position in this embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ef99d-81ad-4628-9940-304728d2c966",
   "metadata": {},
   "source": [
    "Que 23. How do RNN-based techniques handle long-term dependencies in text processing?\n",
    "\n",
    "Ans: RNN's are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, recurrent neural network don't seem to be able to learn them. This problem is called Vanishing gradient problem.\n",
    "\n",
    "The neural network updates the weight using the gradient descent algorithm. The gradients grow smaller when the network progress down to lower layers.The gradients will stay constant meaning there is no space for improvement. The model learns from a change in the gradient. This change affects the network’s output. However, if the difference in the gradients is very small network will not learn anything and so no difference in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f4d8f-d2e1-4b20-8c29-170b0bf62418",
   "metadata": {},
   "source": [
    "Que 24. Explain the concept of sequence-to-sequence models in text processing tasks.\n",
    "\n",
    "Ans: Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84836d-e627-4b59-89c0-a56460cbd3f8",
   "metadata": {},
   "source": [
    "Que 25. What is the significance of attention-based mechanisms in machine translation tasks?\n",
    "\n",
    "Ans:In machine translation, attention mechanism is used to align and selectively focus on relevant parts of the source sentence during the translation process. It allows the model to assign weights to more important words or phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41809c24-b7a9-4fa8-b43d-a47d25f5ae30",
   "metadata": {},
   "source": [
    "Que 26. Discuss the challenges and techniques involved in training generative-based models for text generation.\n",
    "\n",
    "Ans: GAN Problems\n",
    "\n",
    "1) Non-convergence: the model parameters oscillate, destabilize and never converge,\n",
    "\n",
    "2) Mode collapse: the generator collapses which produces limited varieties of samples,\n",
    "\n",
    "3) Diminished gradient: the discriminator gets too successful that the generator gradient vanishes and learns nothing,\n",
    "    \n",
    "Techniques:\n",
    "        Text generation can be addressed with Markov processes or deep generative models like LSTMs. Recently, some of the most advanced methods for text generation include BART, GPT and other GAN-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3217e5-69f2-40ac-9846-7f41a32c8122",
   "metadata": {},
   "source": [
    "Que 27. How can conversation AI systems be evaluated for their performance and effectiveness?\n",
    "\n",
    "Ans: Conversational AI works by combining natural language processing (NLP) and machine learning (ML) processes with conventional, static forms of interactive technology, such as chatbots. This combination is used to respond to users through interactions that mimic those with typical human agents.\n",
    "\n",
    "Conversational AI works by using a combination of natural language processing (NLP) and machine learning (ML). Conversational AI systems are trained on large amounts of data, such as text and speech. This data is used to teach the system how to understand and process human language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5cf04-00a4-4cca-aec4-4283743c468d",
   "metadata": {},
   "source": [
    "Que 28. Explain the concept of transfer learning in the context of text preprocessing.\n",
    "\n",
    "Ans: Transfer learning in a CNN refers to using a pre-trained model on a similar task as a starting point for training a new model on a different task.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Transfer learning allows developers to circumvent the need for lots of new data. A model that has already been trained on a task for which labeled training data is plentiful will be able to handle a new but similar task with far less data. There are other benefits of transfer learning through deep learning as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a506c6-35e4-4811-9dc7-bc018af03465",
   "metadata": {},
   "source": [
    "Que 29. What are some challenges in implementing attention-based mechanisms in text processing models?\n",
    "\n",
    "Ans:However, attention mechanisms also have some challenges and limitations to consider. For instance, they can increase the computational cost and complexity of seq2seq models, by adding more parameters and operations, as well as requiring more memory and time.\n",
    "\n",
    "Additionally, attention mechanisms can suffer from alignment problems and domain mismatches when the input and output sequences have different structures, lengths, vocabularies, domains, or languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6a0d9-377e-4990-999d-2b0a5391bc85",
   "metadata": {},
   "source": [
    "Que 30. Discuss the role of conversation AI in enhancing user experiences and interactions on social media platforms.\n",
    "\n",
    "Ans: Conversational AI adds a layer of intuition and intelligence across communication channels to help provide the modern customer experience your customers crave. And it does so by taking advantage of natural language processing (NLP), machine learning (ML), deep learning, and contextual awareness.\n",
    "\n",
    "AI can also help to improve the accuracy of UX design, by providing more accurate and up-to-date data on user behavior and preferences. In addition, AI can enable personalized and targeted user experiences through the use of machine learning and data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
